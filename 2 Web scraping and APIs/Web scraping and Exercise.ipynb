{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web scraping\n",
    "\n",
    "We will scrape data from:\n",
    "\n",
    "- Internet Movie Database\n",
    "- Washington Post\n",
    "- Wikipedia\n",
    "\n",
    "Ethics:\n",
    "\n",
    "- Scraping can be done much faster than humans. Pause ~1 second before scraping the same site again.\n",
    "- Don't republish other people's data without consent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries\n",
    "\n",
    "If you don't have the library installed already, install them with `pip`. We can do that via Jupyter Notebooks as well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!pip install lxml\n",
    "!pip install BeautifulSoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from lxml import html\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions for scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Scrape all HTML from webpage.\n",
    "def scrapewebpage(url):\n",
    "\t# Open URL and get HTML.\n",
    "\tweb = urllib.request.urlopen(url)\n",
    "\n",
    "\t# Make sure there wasn't any errors opening the URL.\n",
    "\tif (web.getcode() == 200):\n",
    "\t\thtml = web.read()\n",
    "\t\treturn(html)\n",
    "\telse:\n",
    "\t\tprint(\"Error %s reading %s\" % str(web.getcode()), url)\n",
    "\n",
    "# Helper function that scrape the webpage and turn it into soup.\n",
    "def makesoup(url):\n",
    "\thtml = scrapewebpage(url)\n",
    "\treturn(BeautifulSoup(html, \"lxml\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape Internet Movie Database\n",
    "\n",
    "Get some info about a movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Scrape Interstellar (2014) by using our own function \"makesoup\" we defined above.\n",
    "imdb_soup = makesoup('http://www.imdb.com/title/tt0816692/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get movie title.\n",
    "title = imdb_soup.find(itemprop=\"name\").get_text()\n",
    "title = title.strip() # Remove whitespace before and after text\n",
    "\n",
    "# Get movie year.\n",
    "year = imdb_soup.find(id=\"titleYear\").get_text()\n",
    "year = year[1:5] # Remove parentheses, make (2014) into 2014.\n",
    "\n",
    "# Get movie duration.\n",
    "duration = imdb_soup.find(itemprop=\"duration\").get_text()\n",
    "duration = duration.strip() # Remove whitespace before and after text\n",
    "\n",
    "# Get director.\n",
    "director = imdb_soup.find(itemprop=\"director\").get_text()\n",
    "director = director.strip() # Remove whitespace before and after text\n",
    "\n",
    "# Get movie rating.\n",
    "rating = imdb_soup.find(itemprop=\"ratingValue\").get_text()\n",
    "\n",
    "# Get cast list.\n",
    "actors = []\n",
    "for castlist in imdb_soup.find_all(\"table\", \"cast_list\"):\n",
    "\tfor actor in castlist.find_all(itemprop=\"actor\"):\n",
    "\t\tactors.append(actor.get_text().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Present the results.\n",
    "print(\"Movie:    \" + title)\n",
    "print(\"Year:     \" + year)\n",
    "print(\"Director: \" + director)\n",
    "print(\"Duration: \" + duration)\n",
    "print(\"Rating:   \" + rating)\n",
    "\n",
    "# Present list of actors.\n",
    "print()\n",
    "print(\"Main actors:\")\n",
    "for actor in actors:\n",
    "\tprint(\"- \" + actor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape Washington Post\n",
    "\n",
    "Get the latest headlines and links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wpost_soup = makesoup(\"http://www.washingtonpost.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Get headlines.\n",
    "headlines = wpost_soup.find_all(\"div\", \"headline\")\n",
    "print(\"Found \" + str(len(headlines)) + \" headlines\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print headlines.\n",
    "for headline in headlines:\n",
    "    print(headline.get_text().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print headlines and links.\n",
    "for links in headlines:\n",
    "    for link in links.find_all(\"a\"):\n",
    "        print(link.get_text())\n",
    "        print(link.get(\"href\"))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get all the links on the page.\n",
    "for link in wpost_soup.find_all(\"a\"):\n",
    "    href = link.get(\"href\")\n",
    "    if href is not None:\n",
    "        if href[:4] == \"http\":\n",
    "            print(href)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape Wikipedia\n",
    "\n",
    "How many seats does each country have in this conuncil? Scrape a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wp_soup = makesoup(\"https://en.wikipedia.org/wiki/Parliamentary_Assembly_of_the_Council_of_Europe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Lets find the table \"Composition by parliamentary delegation\".\n",
    "\n",
    "# The table doesn't have a unique name, which makes it difficult to scrape.\n",
    "# However, it's the first table. So we can use find, which returns the first match.\n",
    "\n",
    "table = wp_soup.find(\"table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Go through all rows in the table.\n",
    "for row in table.find_all(\"tr\"):\n",
    "    # Go through all cells in each row.\n",
    "    cell = row.find_all(\"td\")\n",
    "    if len(cell) == 3:\n",
    "        # Extract the text from the three cells.\n",
    "        country = cell[0].get_text()\n",
    "        seats = cell[1].get_text()\n",
    "        accessiondate = cell[2].get_text()\n",
    "        print(country + \": \" + seats + \" seats (\" + accessiondate + \")\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "1. Go to <http://www.imdb.com/> and find your favorite movie.\n",
    "2. Try to scrape the rating count (under the rating)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Modify this to your favorite movie.\n",
    "soup = makesoup('http://www.imdb.com/title/tt0816692/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get rating count instead of name.\n",
    "title = soup.find(itemprop=\"name\").get_text()\n",
    "title = title.strip() # Remove whitespace before and after text"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
